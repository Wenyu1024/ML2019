---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---


# Problem 1
## (a)
Note the variance (vector) tried to capture the the variation(distance) of datapoint in 1d, when it comes to 2d the matric becomea the covariance that tries to capture the distance between two variables (the reason that variance is on the diagnol is covariance matrix are variable against variable and the diagnol happens to be the one variable against itself)

let $cov(xr,xs)= x$ ,
since $cov(x_{r},x_{r})$ is just $var(x_{r})$

$-0.75= x/ \sqrt[2]{2\times3}$ \


$x= \sqrt[2]{2\times3} \times (-0.75)$ \

so the $\Sigma$ is 

\begin{matrix}
2 & -1.83711 \\
-1.83711 & 3
\end{matrix}

```{r}
# library(ks)  # for estimatioin
# library(mvtnorm) #for ouput datapoint and density with known distribution
# library(caret)
library(tidyverse)
cov_rs=  (6^0.5* -0.75) 
cov_rs
cov_mat <- matrix(c(2, cov_rs,cov_rs,3  ),nrow = 2,byrow = T)
set.seed(1234)
x <- mvtnorm::rmvnorm(n=200,mean = c(0,0), sigma =  cov_mat )
cov_emp <- cov(x = x)
cov_emp 
cor_emp <- cor(x = x)
cor_emp 

```

## (b)

note the geom_density_2d function first estimates the density out of the given sample, and then connect the contour line of datapoints with same estimated density.

we can break this by first do the estimation and rhen plot contour/perspective plots use separate functions. (kde2 + contour() and persp) 

Or we can use the ggplot interface which combine two steps together:
it seems in ggplot2 it is only possible for the contour plot not perspective plot (without the help of additional packages..)



```{r}
# check <- df %>% tbl_df() %>% 
#   ggplot(aes(x= xr, y= xs)) +stat_contour()

df <- data.frame(xr= x[,1], xs= x[,2])
# p <- df %>% tbl_df() %>% 
#   ggplot(aes(x= xr, y= xs)) +
#   geom_point()+
#   geom_density2d(contour=T)+
#   theme_classic()
# 
# check+geom_contour()

#check <- ks::kde(x = df)
check <- MASS::kde2d(df$xr,df$xs)
contour(check,xlab = "xr", ylab= "xs")
image(check,xlab = "xr", ylab= "xs")
persp(check,xlab = "xr", ylab= "xs",zlab = "Probablity density")


# df %>% tbl_df() %>% 
#   ggplot(aes(x= xr, y= xs)) +
#   geom_point()+
#   geom_countor(contour=T)+
#   theme_classic()

cor.test(df$xr,df$xs)
cov(df)
var(df$xr)
var(df$xs)
```

## (c)
the reason the contour plot is not a circle is that x and y are linear dependent,
and correlation coef is neg (which explains the direction of the contour plot)

```{r}

grid_mat <- expand.grid(.25*(-20:20),.25*(-20:20)) # get input gird coordinates
den_vec = mvtnorm::dmvnorm(x = grid_mat,mean = c(0,0), sigma =  cov_mat ) #get the density

df <- cbind(grid_mat,den_vec) 
colnames(df) <- c("x","y","p")

df %>% 
  ggplot() + 
  geom_point(aes(x=x,y=y),size = 0.1)+
  geom_contour(aes(x=x,y=y,z=p))+
  theme_classic()
  

p_mat= matrix(den_vec,byrow = T,nrow = 41)
contour(x =.25*(-20:20),y= .25*(-20:20), z=p_mat,xlab = "x1", ylab= "x2")
image(x =.25*(-20:20),y= .25*(-20:20), z=p_mat,xlab = "x1", ylab= "x2")
persp(x =.25*(-20:20),y= .25*(-20:20), z=p_mat,xlab = "x1", ylab= "x2",zlab = "Probablity density")
#wide_mat <- pracma::Reshape(grid_mat,*2) 


  

```




```{r}
grid_mat2 <- expand.grid(.25*(-20:20),.25*(-20:20)) # get input gird coordinates
den_vec2 = mvtnorm::dmvnorm(x = grid_mat,,mean = c(2,1), sigma =  cov_mat ) #get the density of these grid points (given a known density function)


p= den_vec/(den_vec2+den_vec)
p_mat2= matrix(p,byrow = T,nrow = 41)
image(x =.25*(-20:20),y= .25*(-20:20),p_mat2,xlab = "x1", ylab= "x2")
```


## (d)
Lets generate data from two classes:
```{r}
x_cls1_mat=  x <- mvtnorm::rmvnorm(n=100,mean = c(0,0), sigma =  cov_mat )
x_cls2_mat=  x <- mvtnorm::rmvnorm(n=100,mean = c(2,1), sigma =  cov_mat )
y1=rep(1,100)
y2=rep(0,100)
x <- rbind(x_cls1_mat,x_cls2_mat)

```

Now lets assume we don't know the outcome for these generated data points,but we have a classifier "trained" on the grid data points which we do have the outcomes.

And we assume each sample data point (the outcomes that we are trying to predict) are observed samples from one of these two distributions.

(Despite that we didn't really do the training but instead construct the  distribution directly with given parameters)

Now Lets construct a function that takes the x1, x2 as input and output p by using the given formula  

```{r}
myclassfier <- function(x, sigma= cov_mat){
  d1= mvtnorm::dmvnorm(x = x,mean = c(0,0), sigma =  sigma )
  d2= mvtnorm::dmvnorm(x = x,mean = c(2,1), sigma =  sigma )
  p = d1*0.5/(d1*0.5+d2*0.5)
  return(p)
}

y_prob= myclassfier(x)
df <- cbind(rbind(x_cls1_mat,x_cls2_mat), (c(y1,y2)),y_prob)
#df[,4] <- apply(df[,1:2], 2, FUN = myclassfier)
df <- data.frame(df)
colnames(df)[1:3] <- c("x1","x2","y_true")



#get the confusion matrix
caret::confusionMatrix(
  data= as.factor(as.integer(df$y_prob > 0.5)),
  reference=as.factor(df$y_true) 
  )




df %>% ggplot(aes(x= x1, y=x2, color=as.factor(y_true))) +
  geom_point()

cor.test(df$x1,df$x2)   #see correlations is no longer -0,75
var(df$x1)
var(df$x2)


contour(df)
```




# Problem2
## (a)
Since \Sigma is a diagnoal matrix and covariant terms equals to 0. we know *X* is a full rank matrix $(x_{1},x_{2}..x_{p})$ where varaible vectors are linear independent of each other.

According to probablity theory

$P(X)= P(x_{1})P(x_{2})....P(x_{p})$

The right term of the equation equals to the left term given in the problem
and the left term of the equation above are just joint probablity (density function) that defined by the right term of the equation in the problem.
So the equation stands.


## (b)

it can be seen that -0.5b is the power term in the exponential function within a,
so if \Sigma is a diagnol matrix , and a=0.01 then
$0.01 \times \frac{2} {\pi} \times \sqrt[2]{1\times2\times3} = e^{-0.5b}$

```{r}
b= log(0.01 * (2* pi)^1.5 *(1*2*3)^0.5) /(-0.5)
b
```



# Problem 3
## (a)
No. it isn't.The author showed that despite the discriminative learning are more likely to be accurate if sample is large enough(approaching the whole population), generative model can actually approaching to the asymototic error much faster despite the error may be higher.
So there are two cases for which model is better given a fixed number of training sample:

1)with smaller training sample,the generative model has already approached its asymptotic error and is thus doing better

2)with sufficiently larget training data, the discriminative model approaches its lower asymptotic error and does better.



## (b) 

Generative model : naive bayes
Discrimitive model: logistic regression


generative model: maximaize the likelihood * prior distribution  (in naives bayes, predictors are considered independent to each other when contributing to repsonse variable)

discrimitive model:  estimate parameters that directly maximize the likelihood P(Y|X) on the training set (in the form of minimizing 0-1 training error)


## (c)
The figures shows that the generative models are easier to converge when increasing the sample size. But in some situation when the sample is sufficiently large the discriminative model performed better, as what has been described in the (a)